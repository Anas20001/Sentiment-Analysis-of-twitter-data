{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![What most of tweets looks like](https://web.stanford.edu/class/cs224n/reports/final_summaries/images/image000.png)","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\n Does most of the tweets on twitter social network are positive, negative, or neutral.\n \nWell, you know what they say ... if you want, you  can find a correlation anywhere you look ... if you're really, deeply paying attention.","metadata":{}},{"cell_type":"markdown","source":"> ðŸŸ¢ <b>Goal</b>: This notebook has the purpose of analysing and predicting twitter tweets if  have positive, negative, or neutral language .","metadata":{}},{"cell_type":"markdown","source":">","metadata":{}},{"cell_type":"markdown","source":"## ðŸ“š Libraries & Functions","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom wordcloud import STOPWORDS as stopwords_wc\n\nimport re\nimport os\n\n# nltk\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# ML & preprocessing tools \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline \nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import roc_curve \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\nimport xgboost as xgb\n\n# Color palette\n\nmy_colors = [\"#ce8f5a\", \"#efd199\", \"#80c8bc\", \"#5ec0ca\", \"#6287a2\"]\nsns.palplot(sns.color_palette(my_colors))\n\n# Set Style\n\nsns.set_style(\"white\")\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['axes.spines.left'] = False\nmpl.rcParams['axes.spines.right'] = False\nmpl.rcParams['axes.spines.top'] = False\n\nclass color:\n    BOLD = '\\033[1m' + '\\033[93m'\n    END = '\\033[0m'\n\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:40.649297Z","iopub.execute_input":"2021-09-23T14:09:40.6496Z","iopub.status.idle":"2021-09-23T14:09:42.474735Z","shell.execute_reply.started":"2021-09-23T14:09:40.649525Z","shell.execute_reply":"2021-09-23T14:09:42.473976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ðŸ“¥ Read in Data","metadata":{}},{"cell_type":"code","source":"col = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndf = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv', header=None, encoding='ISO-8859-1', names = col, skiprows=795000, nrows = 10000)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:42.476253Z","iopub.execute_input":"2021-09-23T14:09:42.476514Z","iopub.status.idle":"2021-09-23T14:09:44.230182Z","shell.execute_reply.started":"2021-09-23T14:09:42.476489Z","shell.execute_reply":"2021-09-23T14:09:44.229431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Map target label to String**\n\n   * 0 -> NEGATIVE\n   * 2 -> NEUTRAL\n   * 4 -> POSITIVE\n\n","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:44.23148Z","iopub.execute_input":"2021-09-23T14:09:44.232113Z","iopub.status.idle":"2021-09-23T14:09:44.238229Z","shell.execute_reply.started":"2021-09-23T14:09:44.23207Z","shell.execute_reply":"2021-09-23T14:09:44.237494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum() # seems like we don't have any null data points ","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:45.607244Z","iopub.execute_input":"2021-09-23T14:09:45.607617Z","iopub.status.idle":"2021-09-23T14:09:45.621618Z","shell.execute_reply.started":"2021-09-23T14:09:45.607582Z","shell.execute_reply":"2021-09-23T14:09:45.620737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"df2 = df[['text', 'target']]\ndf2.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:46.095114Z","iopub.execute_input":"2021-09-23T14:09:46.095402Z","iopub.status.idle":"2021-09-23T14:09:46.113178Z","shell.execute_reply.started":"2021-09-23T14:09:46.095371Z","shell.execute_reply":"2021-09-23T14:09:46.112487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2['target'] = df2['target'].replace(4,1)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:46.354547Z","iopub.execute_input":"2021-09-23T14:09:46.355059Z","iopub.status.idle":"2021-09-23T14:09:46.363414Z","shell.execute_reply.started":"2021-09-23T14:09:46.355021Z","shell.execute_reply":"2021-09-23T14:09:46.362619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Cleaning text column** ","metadata":{}},{"cell_type":"markdown","source":">The process of converting data to something a computer can understand is referred to as pre-processing. One of the major forms of pre-processing is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words. ","metadata":{}},{"cell_type":"markdown","source":"***What are Stop words?***\n\n**Stop Words**: A stop word is a commonly used word (such as â€œtheâ€, â€œaâ€, â€œanâ€, â€œinâ€) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. ","metadata":{}},{"cell_type":"markdown","source":"![](https://media.geeksforgeeks.org/wp-content/cdn-uploads/Stop-word-removal-using-NLTK.png)","metadata":{}},{"cell_type":"code","source":"def emoji_extractor(string, remove=False):\n    '''Removes Emoji from a text.'''\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    if remove == False:\n        # Extract emoji\n        return emoji_pattern.findall(string)\n    else:\n        # Remove emoji from text\n        return emoji_pattern.sub(r'', string)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:47.736555Z","iopub.execute_input":"2021-09-23T14:09:47.736972Z","iopub.status.idle":"2021-09-23T14:09:47.743103Z","shell.execute_reply.started":"2021-09-23T14:09:47.736942Z","shell.execute_reply":"2021-09-23T14:09:47.742459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_emoji(x):\n    if len(x) == 0:\n        return ''\n    else:\n        return x[0] \n    \n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:48.236548Z","iopub.execute_input":"2021-09-23T14:09:48.237037Z","iopub.status.idle":"2021-09-23T14:09:48.241876Z","shell.execute_reply.started":"2021-09-23T14:09:48.236992Z","shell.execute_reply":"2021-09-23T14:09:48.240969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_tweets(df,col):\n    '''Returns the dataframe with the tweet column cleaned.'''\n    \n    # ----- Remove \\n, \\t, \\xa0 -----\n    df[col] = df[col].apply(lambda x: x.replace('\\n', ''))\n    df[col] = df[col].apply(lambda x: x.replace('\\xa0', ''))\n    df[col] = df[col].apply(lambda x: x.replace('\\t', ''))\n    \n    # ----- Remove pic.twitter and http:// + https:// links -----\n    df[col] = df[col].apply(lambda x: re.sub(r'http\\S+', '', x))\n    df[col] = df[col].apply(lambda x: re.sub(r'https\\S+', '', x))\n    df[col] = df[col].apply(lambda x: re.sub(r'pic.twitter\\S+', '', x))\n    \n    # ----- Remove mentions and hashtags -----\n    df[col] = df[col].apply(lambda x: re.sub(r'#\\S+', '', x))\n    df[col] = df[col].apply(lambda x: re.sub(r'@\\S+', '', x))\n    \n    # ----- Extract Emojis and Remove from Tweet -----\n    df['tweet_emojis'] = df[col].apply(lambda x: emoji_extractor(x, remove=False))\n    df['tweet_emojis'].replace('', np.nan, inplace=True)\n#     df[\"tweet_emojis\"] = df[\"tweet_emojis\"].apply(lambda x: clean_emoji(x))\n    \n    df[col] = df[col].apply(lambda x: emoji_extractor(x, remove=True))\n    \n    # ----- Strip of whitespaces -----\n    df[col] = df['tweet'].apply(lambda x: x.strip())\n    df[col] = df[col].apply(lambda x: ' '.join(x.split()))\n    \n    # ----- Remove punctuation & Make lowercase -----\n    df[col] = df[col].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n    df[col] = df[col].apply(lambda x: x.lower())\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:48.277991Z","iopub.execute_input":"2021-09-23T14:09:48.278419Z","iopub.status.idle":"2021-09-23T14:09:48.29206Z","shell.execute_reply.started":"2021-09-23T14:09:48.278378Z","shell.execute_reply":"2021-09-23T14:09:48.291063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEXT CLENAING\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\nstop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:48.430888Z","iopub.execute_input":"2021-09-23T14:09:48.431405Z","iopub.status.idle":"2021-09-23T14:09:48.442279Z","shell.execute_reply.started":"2021-09-23T14:09:48.431366Z","shell.execute_reply":"2021-09-23T14:09:48.441446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_process(text, stem = False):\n    \n    text = re.sub(TEXT_CLEANING_RE, \" \", str(text).lower()).strip()\n    \n    tokens = []\n    \n    for token in text.split():\n        \n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n                \n    return \" \".join(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:48.847316Z","iopub.execute_input":"2021-09-23T14:09:48.847956Z","iopub.status.idle":"2021-09-23T14:09:48.854135Z","shell.execute_reply.started":"2021-09-23T14:09:48.847913Z","shell.execute_reply":"2021-09-23T14:09:48.853404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">","metadata":{}},{"cell_type":"code","source":"%%timeit\n\ndf2['text'] = df2.text.apply(lambda x: text_process(x))","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:49.414488Z","iopub.execute_input":"2021-09-23T14:09:49.415325Z","iopub.status.idle":"2021-09-23T14:09:51.96901Z","shell.execute_reply.started":"2021-09-23T14:09:49.415273Z","shell.execute_reply":"2021-09-23T14:09:51.968151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:51.970672Z","iopub.execute_input":"2021-09-23T14:09:51.97151Z","iopub.status.idle":"2021-09-23T14:09:51.982436Z","shell.execute_reply.started":"2021-09-23T14:09:51.971463Z","shell.execute_reply":"2021-09-23T14:09:51.98152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Most Frequent Words ","metadata":{}},{"cell_type":"code","source":"\n#Make wordcloud\n\nall_tweets = ' '.join(token for token in df['text'])\nstopwords_wc = set(stopwords_wc)\n\nFONT_PATH = \"../input/ace-font/acetone_font.otf\"\n\nwordcloud = WordCloud(stopwords = stopwords_wc, font_path= FONT_PATH,\n                      max_words =1500, \n                      max_font_size = 350, random_state=42,\n                      width = 2000, height=1000,\n                      colormap = 'twilight')\n\nwordcloud.generate(all_tweets)\n\nplt.figure(figsize = (16, 8))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis(\"off\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:09:51.983917Z","iopub.execute_input":"2021-09-23T14:09:51.984192Z","iopub.status.idle":"2021-09-23T14:10:17.524779Z","shell.execute_reply.started":"2021-09-23T14:09:51.984158Z","shell.execute_reply":"2021-09-23T14:10:17.524192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Representing text numerically**\n\n*how we take the **tokenization** we have created and turn them into an array that we can feed into a machine learning algorithm ?*\n\n**Bag of words** is the choice as it is:\n* Simple way to represent text in machine learning.\n* Discards information about grammer and word order.\n* Computes frequency of occurrence.\nassuming that the number of times a word occurs is enough information. \n\n**CountVectorizer()** works by taking an array of strings and doing three things:\n1. Tokenizes all the strings \n2. Builds a \"Vocabulary\" --> makes note of all the words that appear.\n3. Counts the occurrences of each token in the vocabulary.","metadata":{}},{"cell_type":"markdown","source":"![](https://s3.ap-south-1.amazonaws.com/s3.studytonight.com/curious/uploads/pictures/1590391511-1.jpg)","metadata":{}},{"cell_type":"markdown","source":"### **Splitting the dataset into Training and test sets**","metadata":{}},{"cell_type":"code","source":"X = df2['text']\n\ny = df2['target']\n\nX_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:10:17.526498Z","iopub.execute_input":"2021-09-23T14:10:17.526887Z","iopub.status.idle":"2021-09-23T14:10:17.535124Z","shell.execute_reply.started":"2021-09-23T14:10:17.526856Z","shell.execute_reply":"2021-09-23T14:10:17.534611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.values","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:10:17.536283Z","iopub.execute_input":"2021-09-23T14:10:17.536685Z","iopub.status.idle":"2021-09-23T14:10:17.548651Z","shell.execute_reply.started":"2021-09-23T14:10:17.536656Z","shell.execute_reply":"2021-09-23T14:10:17.548091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Creating a pipeline**\n\n\n#### **Pipeline workflow**:\n* Repeatable way to go from raw data to trained model\n\n* Pipeline object takes sequential list of steps, where the output of one step is input to the next.\n\n* Each step is a tuple with two elements: (\"Name of the step\", \"Transform object\")\n\n#### **One-Vs-The-Rest** is a multiclass strategy.\n\n*copying from the official documentation of scikit-learn :*\n\n> Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.","metadata":{}},{"cell_type":"code","source":"pl = Pipeline([('vec', CountVectorizer()), \n               ('clf', OneVsRestClassifier(LogisticRegression()))])\n\npl.fit(X_train, y_train)\n\ny_pred = pl.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:10:17.549868Z","iopub.execute_input":"2021-09-23T14:10:17.550326Z","iopub.status.idle":"2021-09-23T14:10:18.051584Z","shell.execute_reply.started":"2021-09-23T14:10:17.550275Z","shell.execute_reply":"2021-09-23T14:10:18.050668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Pipeline with accuaracy score: \",accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:10:18.052861Z","iopub.execute_input":"2021-09-23T14:10:18.053332Z","iopub.status.idle":"2021-09-23T14:10:18.060391Z","shell.execute_reply.started":"2021-09-23T14:10:18.053272Z","shell.execute_reply":"2021-09-23T14:10:18.059526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is not the best model accuracy score we can get.. Lets try some different preprocessing techniqes to reach higher accuracy for our model.","metadata":{}},{"cell_type":"code","source":"y_pred_proba = pl.predict_proba(X_test)[:,1]\n\nfpr, tpr,thresholds = roc_curve(y_test, y_pred_proba)\n\nplt.plot(fpr, tpr, label = \"Logisitic Regression\")\nplt.plot([0,1], [0,1], \"k--\")","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:10:18.063202Z","iopub.execute_input":"2021-09-23T14:10:18.063898Z","iopub.status.idle":"2021-09-23T14:10:18.396943Z","shell.execute_reply.started":"2021-09-23T14:10:18.063854Z","shell.execute_reply":"2021-09-23T14:10:18.396408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> I used the predicted probabilities of the model assigning a value of (1) to the observation in question. This is because to compute the (ROC) we don't merely want the prdictions on the test set, but we want the probability that our logistic regression model ouputs before using a threshold to predict the label.\n\n#### **Now the question is:** given the ROC curve, can we extract a metric of interset?\n\n* Larger the area under the curve  ==== Better model\n\n**The way to think about this is the following:** if we had a model which produced an ROC curve that had a single point at (1,0) the upper left corner, representing a \"True positive\" rate of one and a \"False positive\" of zero, this will be a great model !\n\n* For this reason the Area under the ROC, commonly denoted as \"AUC\", is another popular metric for classificatioon models. ","metadata":{}},{"cell_type":"code","source":"roc_auc_score(y_test, y_pred_proba)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:10:18.397878Z","iopub.execute_input":"2021-09-23T14:10:18.398185Z","iopub.status.idle":"2021-09-23T14:10:18.406501Z","shell.execute_reply.started":"2021-09-23T14:10:18.398134Z","shell.execute_reply":"2021-09-23T14:10:18.405367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now, when fitting different values of hyperparameters, it is essential to use \"Cross-validation\" as using \"train_test_split\" alone would risk overfitting. ","metadata":{}},{"cell_type":"code","source":"cv_scores = cross_val_score(pl, X, y, cv = 10, scoring = 'roc_auc')\ncv_scores","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:10:18.407953Z","iopub.execute_input":"2021-09-23T14:10:18.408475Z","iopub.status.idle":"2021-09-23T14:10:24.049276Z","shell.execute_reply.started":"2021-09-23T14:10:18.408438Z","shell.execute_reply":"2021-09-23T14:10:24.048535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We could see the difference between using \"train_test_split\" alone and using 'cross_val_score' as we can see that the 9th fold returns with the best model with 89% roc_auc score.","metadata":{}},{"cell_type":"markdown","source":"### **We Could Try differnet way to make sure that we get the most accuarate model.**\n\n>By trying the most known in competitions for this specific problems \"Multiclass classification\"  **XGBOOST**","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, make_scorer\n\n# specifing the parameters \n\nparams = {\n    'max_depth': 6,\n    \n    'objective': 'multi:softmax', ## Error Evaluation For Multivclass Classification\n    \n    'num_class': 3,\n    \n}\n\nparam_grid = {\n    'clf__max_depth': [2, 3, 5, 7, 10],\n    'clf__n_estimators': [10, 100, 500],\n}\n\nclf = xgb.XGBClassifier(**params)\n\npl_2 = Pipeline([('vec', CountVectorizer()), \n               ('clf', clf)])\n\ngrid = GridSearchCV(pl_2, param_grid, cv=5, n_jobs = -1, \n                    scoring = 'accuracy')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T07:08:15.74921Z","iopub.execute_input":"2021-09-19T07:08:15.749721Z","iopub.status.idle":"2021-09-19T07:08:15.760875Z","shell.execute_reply.started":"2021-09-19T07:08:15.749677Z","shell.execute_reply":"2021-09-19T07:08:15.759812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ngrid.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T07:08:15.762883Z","iopub.execute_input":"2021-09-19T07:08:15.763485Z","iopub.status.idle":"2021-09-19T08:26:10.699271Z","shell.execute_reply.started":"2021-09-19T07:08:15.763435Z","shell.execute_reply":"2021-09-19T08:26:10.697913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = grid.predict(X_test)\npred[:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:26:10.702199Z","iopub.execute_input":"2021-09-19T08:26:10.702488Z","iopub.status.idle":"2021-09-19T08:26:10.768533Z","shell.execute_reply.started":"2021-09-19T08:26:10.702427Z","shell.execute_reply":"2021-09-19T08:26:10.76733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, pred))","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:51:02.527457Z","iopub.execute_input":"2021-09-19T08:51:02.527969Z","iopub.status.idle":"2021-09-19T08:51:02.539728Z","shell.execute_reply.started":"2021-09-19T08:51:02.52793Z","shell.execute_reply":"2021-09-19T08:51:02.539016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, pred)\ncm","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:51:22.271713Z","iopub.execute_input":"2021-09-19T08:51:22.272044Z","iopub.status.idle":"2021-09-19T08:51:22.284048Z","shell.execute_reply.started":"2021-09-19T08:51:22.272011Z","shell.execute_reply":"2021-09-19T08:51:22.282937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, normalized=True, cmap='bone'):\n    \n    plt.figure(figsize=[7,6])\n    \n    norm_cm = cm\n    if normalized:\n        norm_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels = classes,\n                    yticklabels = classes, cmap=cmap)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:52:34.108326Z","iopub.execute_input":"2021-09-19T08:52:34.109408Z","iopub.status.idle":"2021-09-19T08:52:34.115388Z","shell.execute_reply.started":"2021-09-19T08:52:34.109347Z","shell.execute_reply":"2021-09-19T08:52:34.114429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(cm, ['pro 1', 'pro 2'])","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:52:49.179931Z","iopub.execute_input":"2021-09-19T08:52:49.180287Z","iopub.status.idle":"2021-09-19T08:52:49.460313Z","shell.execute_reply.started":"2021-09-19T08:52:49.180252Z","shell.execute_reply":"2021-09-19T08:52:49.459392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[](https://bs-uploads.toptal.io/blackfish-uploads/uploaded_file/file/191043/image-1582222692844-bfd251400319962c71d58f464e086281.png)","metadata":{}},{"cell_type":"code","source":"## ![](https://bs-uploads.toptal.io/blackfish-uploads/uploaded_file/file/191043/image-1582222692844-bfd251400319962c71d58f464e086281.png)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://pgirish.github.io/spark-project/index.html\n\nhttps://www.toptal.com/apache/apache-spark-streaming-twitter","metadata":{}},{"cell_type":"markdown","source":"****","metadata":{}}]}